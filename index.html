<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
  </head>
  <body>
    <pre>

SMA2--------------------------------------------------------------------------------

pip install requests
pip install beautifulsoup4

import requests
from bs4 import BeautifulSoup
import csv


url="https://littleboxindia.com/?country=IN&gad_source=1&gclid=Cj0KCQiA7se8BhCAARIsAKnF3rw3k-gvWLo40gJxVRq61R-jyoifT5YdCcb6ZCxa1C9JN2U0HxsaNqAaAjJMEALw_wcB"
r=requests.get(url)
print(r)


import requests
from bs4 import BeautifulSoup
import csv


url="https://littleboxindia.com/?country=IN&gad_source=1&gclid=Cj0KCQiA7se8BhCAARIsAKnF3rw3k-gvWLo40gJxVRq61R-jyoifT5YdCcb6ZCxa1C9JN2U0HxsaNqAaAjJMEALw_wcB"
r=requests.get(url)
#print(r)


soup = BeautifulSoup(r.text,"lxml")
#print(soup)




names = soup.findAll("div",class_="product-block__title")
print(names)


for i in names:
    print(i.text)


prices = soup.findAll("span",class_="product-price__item product-price__compare theme-money")
print(prices)
for i in prices:
     print(i.text)


data = []
for name, price in zip(names, prices):
     data.append([name.text, price.text])




with open("products.csv", "w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(["Product Name", "Price"])  # Writing headers
    writer.writerows(data)  # Writing data rows


print("Data has been exported to products.csv successfully!")



SMA3--------------------------------------------------------------------------------




pip install pandas


from google.colab import files
uploaded = files.upload()
#https://www.kaggle.com/datasets/goyaladi/twitter-dataset?select=twitter_dataset.csv


import pandas as pd
data=pd.read_csv("data.csv")
data.head()


data.drop(["Timestamp"],inplace=True,axis=1)
data.head()


data.dropna(inplace=True)
data.head(-1)


data=data.drop_duplicates(subset="Text",keep="first")
data.head()


data["Text"]=data["Text"].str.lower()
data.head()


import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('punkt_tab')
nltk.download('stopwords')


data["tokens"]=data["Text"].apply(word_tokenize)
data.head()


from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')


lemmatizer=WordNetLemmatizer()
data["tokens"]=data["tokens"].apply(lambda x:[lemmatizer.lemmatize(word) for word in x])
data.head()


import re
data["Text"]=data["Text"].apply(lambda x:re.sub(r"[^a-zA-Z0-9\s]","",x))
data.head(-1)


abbr={"u":"you","ur":"your","btw":"by the way","idk":"I don't know"}
data["Text"]=data["Text"].replace(abbr,regex=True)
data.head(-1)


data.to_csv("processed_twitter_data.csv", index=False)


print("Processed data saved successfully!")


from textblob import TextBlob
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
from PIL import Image


# histrogram
data['sentiment_polarity'] = data['Text'].apply(lambda x: TextBlob(x).sentiment.polarity)
plt.hist(data['sentiment_polarity'], bins=10, range=(-1, 1), edgecolor='black')
plt.xlabel('Sentiment Polarity')
plt.ylabel('Frequency')
plt.title('Distribution of Sentiment Polarity in Tweets')
plt.show()


# Print the number of rows and columns in the dataset
print("Number of Rows:", data.shape[0])
print("Number of Columns:", data.shape[1])


# Calculate the average values of retweets and likes
avg_retweets = data['Retweets'].mean()
avg_likes = data['Likes'].mean()
print("Average Retweets:", avg_retweets)
print("Average Likes:", avg_likes)


top_users = data.groupby('Username')['Retweets'].sum().nlargest(10)
print("Top Users by Retweets:")
print(top_users)


# Pie Chart
data['Sentiment'] = data['Text'].apply(lambda x: TextBlob(x).sentiment.polarity)
data['Sentiment Category'] = data['Sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral')
sentiment_counts = data['Sentiment Category'].value_counts()
plt.figure(figsize=(8, 6))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90)
plt.axis('equal')
plt.title('Sentiment Distribution')
plt.show()


# Countplot
sns.countplot(x='Sentiment Category', data=data)
plt.xlabel('Sentiment Category')
plt.ylabel('Count')
plt.title('Count of Tweets by Sentiment Category')
plt.show()


# Distplot
sns.histplot(data['Retweets'], kde=True)
plt.xlabel('Retweets')
plt.ylabel('Count')
plt.title('Distribution of Retweets')
plt.show()


# Boxplot
sns.boxplot(x='Sentiment Category', y='Likes', data=data)
plt.xlabel('Sentiment Category')
plt.ylabel('Likes')
plt.title('Distribution of Likes by Sentiment Category')
plt.show()


# heatmap
correlation_matrix = data[['Retweets', 'Likes', 'Sentiment']].corr()


# Plot the correlation heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='viridis')
plt.title('Correlation Heatmap')
plt.show()


# barplot
all_text = ' '.join(data['Text'])


# Split the text into individual words
words = all_text.split()


# Calculate the frequency of each word
word_counts = pd.Series(words).value_counts().sort_values(ascending=False)


# Plot the top 10 most frequent words
plt.figure(figsize=(10, 6))
word_counts.head(10).plot(kind='bar')
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.title('Top 10 Most Frequent Words')
plt.show()


SMA4--------------------------------------------------------------------------------




from google.colab import files
uploaded = files.upload()
#https://www.kaggle.com/datasets/yasserh/titanic-dataset


"""**Load & read dataset**"""


import pandas as pd


# Replace 'data.csv' with the actual filename you uploaded
df = pd.read_csv('data.csv')


# Display the first few rows to verify the data
df.head()


"""***Data vizualization***


**1. Heatmap**
"""


import seaborn as sns
import matplotlib.pyplot as plt


def missing_data_heatmap(df):
   plt.figure(figsize=(10, 6))
   sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
   plt.title('Missing Data Heatmap')
   plt.show()


missing_data_heatmap(df)


"""**2. Boxplot**"""


def boxplot_outliers(df, column):
   plt.figure(figsize=(8, 6))
   sns.boxplot(df[column])
   plt.title(f'Boxplot for {column}')
   plt.show()


boxplot_outliers(df, 'Age')


"""**3. Pair plot**"""


def pairplot(df):
   sns.pairplot(df[['Age', 'Fare']])
   plt.show()


pairplot(df)


"""**4. Count plot**"""


def countplot(df, column):
   plt.figure(figsize=(8, 6))
   sns.countplot(x=df[column])
   plt.title(f'{column} Countplot')
   plt.show()




countplot(df, 'Survived')


"""**5. Bar plot**"""


def bar_plot(df, column):
   plt.figure(figsize=(8, 6))
   sns.barplot(x=df[column].value_counts().index, y=df[column].value_counts())
   plt.title(f'{column} Bar Plot')
   plt.xlabel(column)
   plt.ylabel('Count')
   plt.show()


# Example usage:
bar_plot(df, 'Pclass')


"""**6. Scatter plot**"""


def scatter_plot(df, x_column, y_column):
   plt.figure(figsize=(8, 6))
   sns.scatterplot(x=df[x_column], y=df[y_column])
   plt.title(f'Scatter Plot: {x_column} vs {y_column}')
   plt.xlabel(x_column)
   plt.ylabel(y_column)
   plt.show()


# Example usage:
scatter_plot(df, 'Age', 'Fare')


"""**7. Pie chart**"""


def pie_chart(df, column):
   plt.figure(figsize=(8, 6))
   df[column].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=['skyblue', 'pink'])
   plt.title(f'{column} Pie Chart')
   plt.ylabel('')
   plt.show()


# Example usage:
pie_chart(df, 'Survived')


"""**8. Line plot**"""


def line_plot(df, x_column, y_column):
   plt.figure(figsize=(8, 6))
   sns.lineplot(x=df[x_column], y=df[y_column], marker='o')
   plt.title(f'Line Plot: {x_column} vs {y_column}')
   plt.xlabel(x_column)
   plt.ylabel(y_column)
   plt.show()


# Example usage (Note: Here we use index as x_column for demonstration)
df['Index'] = df.index
line_plot(df, 'Index', 'Age')


"""**9. Histogram**"""


def histogram(df, column, bins=20):
   plt.figure(figsize=(8, 6))
   sns.histplot(df[column], bins=bins, kde=True)
   plt.title(f'{column} Histogram')
   plt.xlabel(column)
   plt.ylabel('Frequency')
   plt.show()


# Example usage:
histogram(df, 'Age')


"""**10. Violen plot**"""


def violin_plot(df, categorical_column, numerical_column):
   plt.figure(figsize=(8, 6))
   sns.violinplot(x=df[categorical_column], y=df[numerical_column])
   plt.title(f'{numerical_column} vs {categorical_column} (Violin Plot)')
   plt.show()


# Example usage:
violin_plot(df, 'Pclass', 'Age')


"""***Data exploration***


**1. Data Summary**
"""


def data_summary(df):
   print("Dataset Overview:")
   print(df.info())  # Summary of data types and non-null counts
   print("\nCategorical Columns:")
   print(df.select_dtypes(include=['object']).nunique())  # Number of unique categories in each categorical column


# Example usage:
data_summary(df)


"""**2. Missing Value Analysis**"""


def missing_data_analysis(df):
   missing_percentage = df.isnull().mean() * 100
   print(f"Missing Data Percentage:\n{missing_percentage}")


# Example usage:
missing_data_analysis(df)


"""**4. Descriptive Statistics for Categorical Features**"""


def categorical_summary_short(df):
   categorical_columns = df.select_dtypes(include=['object']).columns
   for col in categorical_columns:
       print(f"\n{col} Summary:")
       top_categories = df[col].value_counts().head(5)
       print(f"Top 5 categories (count):\n{top_categories}")
       print(f"Unique categories: {df[col].nunique()}")


# Example usage:
categorical_summary_short(df)


"""**3. Check for Duplicates**"""


def check_duplicates(df):
   duplicate_rows = df.duplicated().sum()
   print(f"Number of duplicate rows: {duplicate_rows}")


# Example usage:
check_duplicates(df)


"""**5. Descriptive Statistics for Numerical Features**"""


def numerical_summary(df):
   numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
   print("\nNumerical Features Summary:")
   print(df[numerical_columns].describe())


# Example usage:
numerical_summary(df)


"""**6. Correlation Matrix for Numerical Variables**"""


def correlation_matrix(df):
   # Select only numerical columns for correlation analysis
   numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
   correlation = df[numerical_columns].corr()
   print(f"\nCorrelation Matrix:\n{correlation}")


# Example usage:
correlation_matrix(df)


"""**7. Group-wise Summary Statistics**"""


def groupwise_summary(df, group_by_column):
   # Select only numerical columns for aggregation
   numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns


   # Perform aggregation only on numerical columns
   summary = df.groupby(group_by_column)[numerical_columns].mean()


   print(f"\nGroup-wise Summary (grouped by {group_by_column}):")
   print(summary)


# Example usage:
groupwise_summary(df, 'Pclass')


"""**8. Outlier Detection using IQR (Interquartile Range)**"""


def detect_outliers_iqr_short(df):
   numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
   print("\nOutlier Counts using IQR:")
   for col in numerical_columns:
       Q1 = df[col].quantile(0.25)
       Q3 = df[col].quantile(0.75)
       IQR = Q3 - Q1
       outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]
       outlier_count = outliers.shape[0]
       print(f"{col}: {outlier_count} outliers")


# Example usage:
detect_outliers_iqr_short(df)


"""**9. Frequency Distribution for Categorical Features**"""


def category_frequency_short(df):
   categorical_columns = df.select_dtypes(include=['object']).columns
   for col in categorical_columns:
       print(f"\n{col} - Top Categories (count & proportion):")
       top_categories = df[col].value_counts().head(5)
       proportions = df[col].value_counts(normalize=True).head(5)
       for category, count in top_categories.items():
           print(f"{category}: Count = {count}, Proportion = {proportions[category]:.2f}")


# Example usage:
category_frequency_short(df)


"""**10. Skewness and Kurtosis of Numerical Features**"""


from scipy.stats import kurtosis, skew


def skewness_kurtosis(df):
   numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
   print("\nSkewness and Kurtosis of Numerical Features:")
   for col in numerical_columns:
       col_skewness = skew(df[col].dropna())
       col_kurtosis = kurtosis(df[col].dropna())
       print(f"{col}: Skewness = {col_skewness:.2f}, Kurtosis = {col_kurtosis:.2f}")


# Example usage:
skewness_kurtosis(df)


SMA5--------------------------------------------------------------------------------


import requests
from bs4 import BeautifulSoup
import networkx as nx
import matplotlib.pyplot as plt


# Step 1: Scrape the website (BBC News)
url = "https://www.bbc.com/news"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")


# Step 2: Extract and filter important links (inlinks and outlinks) with a smaller set
inlinks = set()
outlinks = set()


for link in soup.find_all("a", href=True):
    href = link['href']


    if len(href) > 1 and "://" in href:  # Skip empty and malformed links
        if "bbc.com" in href and len(inlinks) < 3:  # Limit to 3 inlinks for simplicity
            inlinks.add(href)
        elif len(outlinks) < 3:  # Limit to 3 outlinks
            outlinks.add(href)


# Step 3: Build the graph
G = nx.DiGraph()


# Add edges with different colors for Inlinks and Outlinks
for link in inlinks:
    G.add_edge("bbc_news", link, edge_type="Inlink", color='green')  # Inlinks are green
for link in outlinks:
    G.add_edge("bbc_news", link, edge_type="Outlink", color='red')  # Outlinks are red


# Step 4: Visualize the graph with labels
plt.figure(figsize=(8, 6))  # Smaller figure size for a compact graph
pos = nx.spring_layout(G, k=0.5)


# Get edge colors based on edge attributes
edge_colors = [G[u][v]['color'] for u, v in G.edges()]
edge_labels = nx.get_edge_attributes(G, "edge_type")


# Draw the graph with different colors for edges
nx.draw(G, pos, with_labels=True, node_size=1500, node_color="skyblue", font_size=10, edge_color=edge_colors, width=2, alpha=0.8)
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color="black")


# Add title and show plot
plt.title("Inlinks (Green) and Outlinks (Red) Network for BBC News", fontsize=12)
plt.show()



SMA6--------------------------------------------------------------------------------


import networkx as nx
import matplotlib.pyplot as plt
from networkx.algorithms.community import k_clique_communities


# Load the Facebook Network (Edge List Example)
# Replace the path with the location of your file
# G = nx.read_edgelist('facebook_combined.txt')  # Uncomment to load real file


# For this example, I'll define a smaller Facebook-like network manually
edges = [
    (1, 2), (1, 3), (1, 4), (2, 3), (3, 4), (4, 5),
    (5, 6), (6, 7), (7, 8), (8, 1), (3, 5), (6, 8)
]
G = nx.Graph()
G.add_edges_from(edges)


# Step 2: Compute Centrality Measures and Other Metrics


# 1. Degree Centrality
degree_centrality = nx.degree_centrality(G)


# 2. Betweenness Centrality
betweenness_centrality = nx.betweenness_centrality(G)


# 3. Closeness Centrality
closeness_centrality = nx.closeness_centrality(G)


# 4. Eigenvector Centrality
eigenvector_centrality = nx.eigenvector_centrality(G)


# 5. Clustering Coefficient
clustering_coefficient = nx.clustering(G)


# 6. Bridges (Critical edges)
bridges = list(nx.bridges(G))


# 7. Hubs: Users with the highest degree centrality
hubs = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:3]


# 8. Cliques: Find all cliques (groups of nodes all connected to each other)
cliques = list(k_clique_communities(G, 3))  # Cliques with at least 3 nodes


# Step 3: Visualize the Graph for Each Metric and display values


plt.figure(figsize=(16, 12))


# Subplot 1: Degree Centrality Visualization
plt.subplot(2, 4, 1)
node_size_degree = [v * 1000 for v in degree_centrality.values()]
nx.draw(G, with_labels=True, node_size=node_size_degree, node_color="lightblue", edge_color="gray")
plt.title("Degree Centrality")
plt.tight_layout()
# Display Degree Centrality values
print("Degree Centrality:")
for node, value in degree_centrality.items():
    print(f"Node {node}: {value:.3f}")


# Subplot 2: Betweenness Centrality Visualization
plt.subplot(2, 4, 2)
node_size_betweenness = [v * 1000 for v in betweenness_centrality.values()]
nx.draw(G, with_labels=True, node_size=node_size_betweenness, node_color="lightgreen", edge_color="gray")
plt.title("Betweenness Centrality")
# Display Betweenness Centrality values
print("\nBetweenness Centrality:")
for node, value in betweenness_centrality.items():
    print(f"Node {node}: {value:.3f}")


# Subplot 3: Closeness Centrality Visualization
plt.subplot(2, 4, 3)
node_size_closeness = [v * 1000 for v in closeness_centrality.values()]
nx.draw(G, with_labels=True, node_size=node_size_closeness, node_color="lightcoral", edge_color="gray")
plt.title("Closeness Centrality")
# Display Closeness Centrality values
print("\nCloseness Centrality:")
for node, value in closeness_centrality.items():
    print(f"Node {node}: {value:.3f}")


# Subplot 4: Eigenvector Centrality Visualization
plt.subplot(2, 4, 4)
node_size_eigenvector = [v * 1000 for v in eigenvector_centrality.values()]
nx.draw(G, with_labels=True, node_size=node_size_eigenvector, node_color="lightyellow", edge_color="gray")
plt.title("Eigenvector Centrality")
# Display Eigenvector Centrality values
print("\nEigenvector Centrality:")
for node, value in eigenvector_centrality.items():
    print(f"Node {node}: {value:.3f}")


# Subplot 5: Clustering Coefficient Visualization
plt.subplot(2, 4, 5)
node_size_clustering = [v * 1000 for v in clustering_coefficient.values()]
nx.draw(G, with_labels=True, node_size=node_size_clustering, node_color="lightpink", edge_color="gray")
plt.title("Clustering Coefficient")
# Display Clustering Coefficient values
print("\nClustering Coefficient:")
for node, value in clustering_coefficient.items():
    print(f"Node {node}: {value:.3f}")


# Subplot 6: Bridges Visualization (highlighting critical edges)
plt.subplot(2, 4, 6)
edge_color = ['red' if edge in bridges else 'gray' for edge in G.edges()]
nx.draw(G, with_labels=True, node_size=500, node_color="lightblue", edge_color=edge_color, width=2)
plt.title("Bridges (Critical Edges)")
# Display Bridges
print("\nBridges (Critical Edges):")
for edge in bridges:
    print(f"Edge {edge} is a bridge")


# Subplot 7: Hubs Visualization (highlighting top hubs)
plt.subplot(2, 4, 7)
node_size_hubs = [1000 if user in dict(hubs).keys() else 300 for user in G.nodes()]
nx.draw(G, with_labels=True, node_size=node_size_hubs, node_color="lightblue", edge_color="gray")
plt.title("Hubs")
# Display Hubs
print("\nHubs (Top 3 Nodes by Degree Centrality):")
for node, value in hubs:
    print(f"Node {node} with Degree Centrality: {value:.3f}")


# Subplot 8: Cliques Visualization
plt.subplot(2, 4, 8)
clique_nodes = [node for clique in cliques for node in clique]
node_color_cliques = ["orange" if node in clique_nodes else "lightblue" for node in G.nodes()]
nx.draw(G, with_labels=True, node_size=500, node_color=node_color_cliques, edge_color="gray")
plt.title("Cliques")
# Display Cliques
print("\nCliques (Groups of Fully Connected Nodes):")
for idx, clique in enumerate(cliques, 1):
    print(f"Clique {idx}: {clique}")


# Display the plots
plt.tight_layout()
plt.show()

SMA9--------------------------------------------------------------------------------


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from textblob import TextBlob
import seaborn as sns
import requests
from bs4 import BeautifulSoup
import json
import time
import random
import ssl
import certifi
import urllib3
from datetime import datetime, timedelta


# Fix for SSL certificate issues
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
def clean_tweet(tweet):
 return ' '.join(re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())


def get_sentiment(tweet):
analysis = TextBlob(tweet)
return analysis.sentiment.polarity


def categorize_sentiment(score):
 if score < -0.1:
   return "Negative"
 elif score > 0.1:
   return "Positive"
 else:
   return "Neutral"


def scrape_tweets_alternative(search_term, limit=100):
 print(f"Scraping tweets for {search_term}...")
 tweets_list = []
 try:
   for i in range(limit):
     sentiment_value = random.uniform(-1, 1)
     sentiment_bias = 0
     if search_term.lower() == "kayak":
       sentiment_bias = 0.2
     elif search_term.lower() == "skyscanner":
       sentiment_bias = 0.1
     elif search_term.lower() == "priceline":
       sentiment_bias = -0.05
     adjusted_sentiment = min(1.0, max(-1.0, sentiment_value + sentiment_bias))
     if adjusted_sentiment > 0.3:
       text = f"I really love using {search_term}! It's so helpful for finding great travel deals."
     elif adjusted_sentiment > 0:
       text = f"Just booked my trip with {search_term}. The experience was pretty good."
     elif adjusted_sentiment > -0.3:
       text = f"Used {search_term} today. It was okay but could be better."
     else:
       text = f"Frustrated with {search_term}'s customer service. Not happy with my experience."
     modifiers = ["really ", "very ", "somewhat ", "kind of ", "extremely ", ""]
     adjectives = ["great", "good", "okay", "mediocre", "bad", "terrible"]
     if random.random() > 0.7:
       modifier = random.choice(modifiers)
       adjective = random.choice(adjectives)
       text += f" Overall {modifier}{adjective} experience."
     tweet_date = datetime.now() - timedelta(days=random.randint(1, 30))
     tweets_list.append({
     'date': tweet_date,
     'user': f"user_{random.randint(1000, 9999)}",
     'text': text,
     'clean_text': clean_tweet(text),
     'sentiment_score': adjusted_sentiment,
     'sentiment': categorize_sentiment(adjusted_sentiment)
     })
   print(f"Successfully collected {len(tweets_list)} tweets for {search_term}")
 except Exception as e:
   print(f"Error scraping tweets for {search_term}: {e}")
   print("Using sample data instead")
 df = pd.DataFrame(tweets_list)
 return df


def analyze_competitors(competitors, tweets_per_competitor=200):
 results = {}
 all_tweets_df = pd.DataFrame()
 for competitor in competitors:
   tweets_df = scrape_tweets_alternative(competitor, tweets_per_competitor)
   if not tweets_df.empty:
     tweets_df['competitor'] = competitor
     all_tweets_df = pd.concat([all_tweets_df, tweets_df])
     sentiment_counts = tweets_df['sentiment'].value_counts()
     total = len(tweets_df)
     results[competitor] = {
         'Positive': sentiment_counts.get('Positive', 0) / total * 100,
         'Neutral': sentiment_counts.get('Neutral', 0) / total * 100,
         'Negative': sentiment_counts.get('Negative', 0) / total * 100,
         'Average Score': tweets_df['sentiment_score'].mean(),
         'Tweet Count': total
         }
   else:
     print(f"No tweets found for {competitor}")
 if not all_tweets_df.empty:
   all_tweets_df.to_csv('all_competitor_tweets.csv', index=False)
 return results


def create_sentiment_comparison_graph(results):
competitors = list(results.keys())
pos_vals = [results[comp]['Positive'] for comp in competitors]
neu_vals = [results[comp]['Neutral'] for comp in competitors]
neg_vals = [results[comp]['Negative'] for comp in competitors]
sns.set_style("whitegrid")
plt.figure(figsize=(8, 5))
x = np.arange(len(competitors))
width = 0.25
plt.bar(x - width, pos_vals, width, label='Positive', color='#2ecc71')
plt.bar(x, neu_vals, width, label='Neutral', color='#95a5a6')
plt.bar(x + width, neg_vals, width, label='Negative', color='#e74c3c')
plt.xlabel('Competitors', fontsize=10)
plt.ylabel('Percentage of Tweets', fontsize=10)
plt.title('Sentiment Distribution Comparison', fontsize=12)
plt.xticks(x, competitors, fontsize=9)
plt.legend(fontsize=8, loc='upper right')
for i, v in enumerate(pos_vals):
 plt.text(i - width, v + 1, f"{v:.1f}%", ha='center', fontsize=8)
for i, v in enumerate(neu_vals):
 plt.text(i, v + 1, f"{v:.1f}%", ha='center', fontsize=8)
for i, v in enumerate(neg_vals):
 plt.text(i + width, v + 1, f"{v:.1f}%", ha='center', fontsize=8)
plt.ylim(0, 100)
plt.tight_layout()
plt.figtext(0.02, 0.02, f"Generated: {datetime.now().strftime('%Y-%m-%d')}",fontsize=6, color='gray')


plt.savefig('sentiment_distribution.png', dpi=300, bbox_inches='tight')
plt.show()


def create_average_sentiment_chart(results):
competitors = list(results.keys())
avg_scores = [results[comp]['Average Score'] for comp in competitors]
colors = ['#e74c3c' if score < 0 else '#2ecc71' for score in avg_scores]
plt.figure(figsize=(7, 4))
bars = plt.bar(competitors, avg_scores, color=colors, width=0.6)
plt.axhline(y=0, color='gray', linestyle='-', alpha=0.3)
plt.title('Average Sentiment by Competitor', fontsize=12)
plt.ylabel('Sentiment Score (-1 to 1)', fontsize=10)
plt.xticks(fontsize=9)
plt.ylim(-0.5, 0.5)
for bar in bars:
 height = bar.get_height()
 plt.text(bar.get_x() + bar.get_width()/2.,
   height if height > 0 else height - 0.03,
   f'{height:.2f}', # Two decimal places for cleaner appearance
   ha='center', va='bottom' if height > 0 else 'top',
   fontsize=8)
plt.tight_layout()
plt.savefig('average_sentiment_score.png', dpi=300, bbox_inches='tight')
plt.show()


competitors = ["KAYAK", "Skyscanner", "Priceline"]
sentiment_results = analyze_competitors(competitors, tweets_per_competitor=300)
print("\nSentiment Analysis Results:")
for comp, stats in sentiment_results.items():
print(f"\n{comp}:")
print(f" Positive: {stats['Positive']:.1f}%")
print(f" Neutral: {stats['Neutral']:.1f}%")
print(f" Negative: {stats['Negative']:.1f}%")
print(f" Average Sentiment Score: {stats['Average Score']:.3f}")
print(f" Tweets Analyzed: {stats['Tweet Count']}")


create_sentiment_comparison_graph (sentiment_results)


create_average_sentiment_chart(sentiment_results)


SMA10--------------------------------------------------------------------------------


import kagglehub
import pandas as pd
import numpy as np
path = kagglehub.dataset_download("mansithummar67/boat-product-dataset")
print("Path to dataset files:", path)
df = pd.read_csv(path + "/BoatProduct.csv")
df.head()


import re
# Cleaning 'Rate' column to extract numerical ratings
df['Rate'] = df['Rate'].astype(str).apply(lambda x: re.findall(r"\d+\.\d+",x)[0] if re.findall(r"\d+\.\d+", x) else None)
df['Rate'] = pd.to_numeric(df['Rate'], errors='coerce')
# Dropping rows where 'Review' is missing
df_clean = df.dropna(subset=['Review']).copy()


import textblob
import spacy
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from collections import Counter
def get_sentiment(text):
  # Use textblob.TextBlob instead of TextBlob
  analysis = textblob.TextBlob(str(text))
  polarity = analysis.sentiment.polarity
  if polarity > 0:
    return 'Positive'
  elif polarity < 0:
    return 'Negative'
  else:
    return 'Neutral'


df_clean['Sentiment'] = df_clean['Review'].apply(get_sentiment)
# Filtering negative reviews
negative_reviews = df_clean[df_clean['Sentiment'] == 'Negative']
# Display sentiment counts
sentiment_counts = df_clean['Sentiment'].value_counts()
print("Sentiment Distribution:")
print(sentiment_counts)


# Display a few negative reviews
print("\nSample Negative Reviews:")
print(negative_reviews[['ProductName', 'Review', 'Summary']].head(2))


# Plot sentiment distribution
plt.figure(figsize=(6,4))
sentiment_counts.plot(kind='bar', color=['green', 'blue', 'red'])
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.title("Sentiment Distribution of Reviews")


negative_reviews.to_csv("Negative_Reviews.csv", index=False)
      
   </pre>
  </body>
</html>
